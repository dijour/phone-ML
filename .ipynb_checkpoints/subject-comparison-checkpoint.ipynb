{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Activity Recognition\n",
    "The goal is to develop a machine learning pipeline to recognize different social media usage using an iPhoneâ€™s motion sensors (accelerometer, gyroscope, etc).\n",
    "\n",
    "#### Activities\n",
    "1. Instagram\n",
    "2. Tinder\n",
    "3. Facebook\n",
    "4. LinkedIn\n",
    "5. Notetaking/texting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import numpy as np # used for handling numbers\n",
    "import pandas as pd # used for handling the dataset\n",
    "import os # to get csv files of data\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.impute import SimpleImputer # used for handling missing data\n",
    "from sklearn.preprocessing import StandardScaler # used for feature scaling\n",
    "\n",
    "# classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the sensor data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_folder, activity, convert_to_numpy=True, graph=False):\n",
    "    out = []\n",
    "    file_names = os.listdir('%s/%s' %(base_folder, activity))\n",
    "    for file in file_names:\n",
    "        if (file == '.DS_Store'):\n",
    "            continue\n",
    "        data = pd.read_csv(('%s/%s/'+file) % (base_folder, activity),\n",
    "                            usecols=[\n",
    "                                    'loggingTime(txt)',\n",
    "                                    'loggingSample(N)', # this is easier to filter beginning/end frames\n",
    "                                    'locationLatitude(WGS84)',\n",
    "                                    'locationLongitude(WGS84)',\n",
    "                                    'locationAltitude(m)',\n",
    "                                    'locationSpeed(m/s)',\n",
    "                                    'accelerometerAccelerationX(G)',\n",
    "                                    'accelerometerAccelerationY(G)',\n",
    "                                    'accelerometerAccelerationZ(G)',\n",
    "                                    'gyroRotationX(rad/s)',\n",
    "                                    'gyroRotationY(rad/s)',\n",
    "                                    'gyroRotationZ(rad/s)',\n",
    "                                    'motionYaw(rad)',\n",
    "                                    'motionRoll(rad)',\n",
    "                                    'motionPitch(rad)',\n",
    "                                    'motionRotationRateX(rad/s)',\n",
    "                                    'motionRotationRateY(rad/s)',\n",
    "                                    'motionRotationRateZ(rad/s)',\n",
    "                                    'motionUserAccelerationX(G)',\n",
    "                                    'motionUserAccelerationY(G)',\n",
    "                                    'motionUserAccelerationZ(G)',\n",
    "                                    'motionQuaternionX(R)',\n",
    "                                    'motionQuaternionY(R)',\n",
    "                                    'motionQuaternionZ(R)',\n",
    "                                    'motionQuaternionW(R)',\n",
    "                                    'motionGravityX(G)',\n",
    "                                    'motionGravityY(G)',\n",
    "                                    'motionGravityZ(G)',\n",
    "                                    'altimeterRelativeAltitude(m)',\n",
    "                                    'altimeterPressure(kPa)',\n",
    "                                    'deviceOrientation(Z)',\n",
    "                                    'avAudioRecorderPeakPower(dB)',\n",
    "                                    'avAudioRecorderAveragePower(dB)']\n",
    "                          )\n",
    "        data = filter_input(data, graph)\n",
    "        \n",
    "        if convert_to_numpy:\n",
    "            out.append(data.to_numpy())\n",
    "        else:\n",
    "            out.append(data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful not to run the plots when loading all the data. Test it by loading just a file or group of files and uncomment after finished exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a frequency filter on data\n",
    "def freq_filter(data, f_size, cutoff):\n",
    "    lgth, num_signal=data.shape\n",
    "    f_data=np.zeros([lgth, num_signal])\n",
    "    lpf=signal.firwin(f_size, cutoff, window='hamming')\n",
    "    for i in range(num_signal):\n",
    "        f_data[:,i]=signal.convolve(data[:,i], lpf, mode='same')\n",
    "    return f_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters accelerometer or gyroscope data\n",
    "cutoff=10\n",
    "fs=512\n",
    "def filter_sensor(data, sensor_type, graph=False):\n",
    "    if sensor_type == \"gyroscope\":\n",
    "        sensor_data = data.iloc[:,9:12].to_numpy()\n",
    "    else:\n",
    "        sensor_data = data.iloc[:,6:9].to_numpy()\n",
    "    \n",
    "    lpf_data=freq_filter(sensor_data, 155, cutoff/fs)\n",
    "    \n",
    "    if graph:\n",
    "        median_data=median_filter(sensor_data, 155)\n",
    "        comb_data=freq_filter(median_data, 155, cutoff/fs)\n",
    "\n",
    "        plot_lines(sensor_data, fs, 'Raw data')\n",
    "        fft_plot(sensor_data, fs, 'Raw data')\n",
    "\n",
    "        plot_lines(median_data, fs, 'median filter')\n",
    "        plot_lines(lpf_data, fs, 'low pass filter')\n",
    "        plot_lines(comb_data, fs, 'median+low pass filter')\n",
    "\n",
    "        fft_plot(lpf_data, fs, 'low pass filter')\n",
    "        fft_plot(median_data, fs, 'median filter')\n",
    "        fft_plot(comb_data, fs, 'median+low pass filter')\n",
    "\n",
    "        plot3D(sensor_data, 'raw data')\n",
    "        plot3D(median_data, 'median filter')\n",
    "        plot3D(lpf_data, 'low pass filter')\n",
    "        plot3D(comb_data, 'median+low pass filter')\n",
    "        plt.show()\n",
    "    \n",
    "    if sensor_type == \"gyroscope\":\n",
    "        data.loc[:,'gyroRotationX(rad/s)'] = lpf_data[:,0]\n",
    "        data.loc[:,'gyroRotationY(rad/s)'] = lpf_data[:,1]\n",
    "        data.loc[:,'gyroRotationZ(rad/s)'] = lpf_data[:,2]\n",
    "    else:\n",
    "        data.loc[:,'accelerometerAccelerationX(G)'] = lpf_data[:,0]\n",
    "        data.loc[:,'accelerometerAccelerationY(G)'] = lpf_data[:,1]\n",
    "        data.loc[:,'accelerometerAccelerationZ(G)'] = lpf_data[:,2]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first 2 seconds (60 rows), and last 1 second (30 rows)\n",
    "def filter_input(data, graph=False):\n",
    "    data = data.iloc[60:-60,]\n",
    "    data = filter_sensor(data, \"accelerometer\", graph)\n",
    "    data = filter_sensor(data, \"gyroscope\", graph)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to examine and compare all of the plots here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-38d1a8734ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load subject 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_tinder_sub1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_subject_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tinder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_instagram_sub1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_subject_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"instagram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_notes_sub1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_subject_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"notes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_facebook_sub1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_subject_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"facebook\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ebd3a8404056>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(base_folder, activity, convert_to_numpy, graph)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                     'avAudioRecorderAveragePower(dB)']\n\u001b[1;32m     42\u001b[0m                           )\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-945e0fa35c2d>\u001b[0m in \u001b[0;36mfilter_input\u001b[0;34m(data, graph)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfilter_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_sensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerometer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_sensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gyroscope\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-842b3482c8ae>\u001b[0m in \u001b[0;36mfilter_sensor\u001b[0;34m(data, sensor_type, graph)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msensor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlpf_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensor_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m155\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-44f7034de30e>\u001b[0m in \u001b[0;36mfreq_filter\u001b[0;34m(data, f_size, cutoff)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlgth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_signal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_signal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlpf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirwin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hamming'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'signal' is not defined"
     ]
    }
   ],
   "source": [
    "# Load subject 1\n",
    "X_tinder_sub1 = load_data(\"data_subject_1\", \"tinder\")\n",
    "X_instagram_sub1 = load_data(\"data_subject_1\", \"instagram\")\n",
    "X_notes_sub1 = load_data(\"data_subject_1\", \"notes\")\n",
    "X_facebook_sub1 = load_data(\"data_subject_1\", \"facebook\")\n",
    "X_linkedin_sub1 = load_data(\"data_subject_1\", \"linkedin\")\n",
    "\n",
    "# Assigning groundtruth conditions to each class\n",
    "Y_tinder_sub1 = [0] * len(X_tinder_sub1) \n",
    "Y_instagram_sub1 = [1] * len(X_instagram_sub1)\n",
    "Y_notes_sub1 = [2] * len(X_notes_sub1)\n",
    "Y_facebook_sub1 = [3] * len(X_facebook_sub1)\n",
    "Y_linkedin_sub1 = [4] * len(X_linkedin_sub1)\n",
    "\n",
    "X_sub1 = np.concatenate((X_tinder_sub1, X_instagram_sub1, X_notes_sub1, X_facebook_sub1, X_linkedin_sub1)) # insert standing when done \n",
    "Y_sub1 = np.concatenate((Y_tinder_sub1, Y_instagram_sub1, Y_notes_sub1, Y_facebook_sub1, Y_linkedin_sub1)) # insert standing when done\n",
    "print(\"Total number of X samples: \" + str(len(X_sub1)))\n",
    "print(\"Total number of Y samples: \" + str(len(Y_sub1)))\n",
    "\n",
    "# Load subject 2\n",
    "X_tinder_sub2 = load_data(\"data_subject_2\", \"tinder\")\n",
    "X_instagram_sub2 = load_data(\"data_subject_2\", \"instagram\")\n",
    "X_notes_sub2 = load_data(\"data_subject_2\", \"notes\")\n",
    "X_facebook_sub2 = load_data(\"data_subject_2\", \"facebook\")\n",
    "X_linkedin_sub2 = load_data(\"data_subject_2\", \"linkedin\")\n",
    "\n",
    "# Assigning groundtruth conditions to each class\n",
    "Y_tinder_sub2 = [0] * len(X_tinder_sub2)\n",
    "Y_instagram_sub2 = [1] * len(X_instagram_sub2)\n",
    "Y_notes_sub2 = [2] * len(X_notes_sub2)\n",
    "Y_facebook_sub2 = [3] * len(X_facebook_sub2)\n",
    "Y_linkedin_sub2 = [4] * len(X_linkedin_sub2)\n",
    "\n",
    "X_sub2 = np.concatenate((X_tinder_sub2, X_instagram_sub2, X_notes_sub2, X_facebook_sub2, X_linkedin_sub2)) # insert standing when done \n",
    "Y_sub2 = np.concatenate((Y_tinder_sub2, Y_instagram_sub2, Y_notes_sub2, Y_facebook_sub2, Y_linkedin_sub2)) # insert standing when done\n",
    "print(\"Total number of X samples: \" + str(len(X_sub2)))\n",
    "print(\"Total number of Y samples: \" + str(len(Y_sub2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rootmean square\n",
    "def rootmean_square(input_data):\n",
    "    return ((1 / len(input_data)) * np.sum([i ** 2 for i in input_data]))**0.5\n",
    "\n",
    "# calculate the norm/magnitude of a 3-axis sensor\n",
    "def calc_sensor_norm(sensor_x, sensor_y, sensor_z):\n",
    "    sensor_total = []\n",
    "    for i in range(len(sensor_x)):\n",
    "        sensor_total.append((sensor_x[i]**2 + sensor_y[i]**2 + sensor_z[i]**2)**5)\n",
    "    return sensor_total\n",
    "\n",
    "# window input data by seconds and overlap\n",
    "def window_input(input_data, seconds=1, overlap=0.5, hz=30):\n",
    "    data_len = len(input_data)\n",
    "    window_len = seconds*hz\n",
    "    overlap_len = int(window_len * (1-overlap))\n",
    "    frame_count = 0\n",
    "    windows, next_win = [], []\n",
    "    # create windows if fits; up to overlap_len - 1 frames removed from end\n",
    "    for i in range(data_len):\n",
    "        next_win.append(input_data[i])\n",
    "        frame_count += 1\n",
    "        if (frame_count % overlap_len == 0) and (len(next_win) == window_len):\n",
    "            windows.append(next_win.copy())\n",
    "            del next_win[:overlap_len]\n",
    "    windows = np.asarray(windows)\n",
    "    return np.asarray(windows)\n",
    "\n",
    "# create spectrogram data. If windowing, calculate FFT at 50% overlap; else use whole recording\n",
    "def process_input(X, Y, windowing):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    groups = list([])\n",
    "    for i in range(len(X)):\n",
    "        if windowing:\n",
    "            windows = window_input(X[i], 2, 0.5, 30)\n",
    "            # featurize and add to output\n",
    "            for w in windows:\n",
    "                groups.append(i)\n",
    "                a_feat = featurize_input(w)\n",
    "                x_out.append(a_feat)\n",
    "                y_out.append(Y[i])\n",
    "        else:\n",
    "            groups.append(i)\n",
    "            w = featurize_input(X[i])\n",
    "            x_out.append(w)\n",
    "            y_out.append(Y[i])\n",
    "    return (np.asarray(x_out), np.asarray(y_out), np.asarray(groups))\n",
    "\n",
    "# Add bins as features or domain-specific features\n",
    "def featurize_input(sample):\n",
    "    fv = []\n",
    "    acc_x = sample[:,6]\n",
    "    acc_y = sample[:,7]\n",
    "    acc_z = sample[:,8]\n",
    "    acc_total = calc_sensor_norm(acc_x, acc_y, acc_z)\n",
    "    gyro_x = sample[:,9]\n",
    "    gyro_y = sample[:,10]\n",
    "    gyro_z = sample[:,11]\n",
    "    gyro_total = calc_sensor_norm(gyro_x, gyro_y, gyro_z)\n",
    "    loc_speed = sample[:,5]\n",
    "    altimeter = sample[:,28]\n",
    "    motionYaw = sample[:,12]\n",
    "    motionRoll = sample[:,13]\n",
    "    motionPitch = sample[:,14]\n",
    "    motionRotationRateX = sample[:,15]\n",
    "    motionRotationRateY = sample[:,16]\n",
    "    motionRotationRateZ = sample[:,17]\n",
    "    \n",
    "    # extremes\n",
    "    fv.append(np.max(acc_total))\n",
    "    fv.append(np.max(gyro_total))\n",
    "    fv.append(np.max(loc_speed))\n",
    "    fv.append(np.max(altimeter))\n",
    "    fv.append(np.max(motionYaw))\n",
    "    fv.append(np.max(motionRoll))\n",
    "    \n",
    "    fv.append(np.min(motionPitch))\n",
    "    \n",
    "    fv.append(np.max(gyro_x))\n",
    "    fv.append(np.max(gyro_y))\n",
    "    fv.append(np.max(gyro_z))\n",
    "\n",
    "    # averages\n",
    "    fv.append(np.mean(acc_total))\n",
    "    fv.append(np.mean(gyro_total))\n",
    "    fv.append(np.mean(loc_speed))\n",
    "    fv.append(np.mean(acc_x))\n",
    "    fv.append(np.mean(acc_y))\n",
    "    fv.append(np.mean(acc_z))\n",
    "    fv.append(np.mean(gyro_x))\n",
    "    fv.append(np.mean(gyro_y))\n",
    "    fv.append(np.mean(gyro_z))\n",
    "    fv.append(np.mean(motionYaw))\n",
    "    fv.append(np.mean(motionRoll))\n",
    "    \n",
    "    fv.append(np.median(acc_total))\n",
    "    fv.append(np.median(gyro_total))\n",
    "    fv.append(np.median(loc_speed))\n",
    "    \n",
    "    # variance\n",
    "    fv.append(np.std(acc_total))\n",
    "    fv.append(np.std(gyro_total))\n",
    "    fv.append(np.std(loc_speed))\n",
    "    fv.append(np.std(altimeter))\n",
    "    fv.append(np.std(motionYaw))\n",
    "    fv.append(np.std(motionRoll))\n",
    "    \n",
    "    fv = np.asarray(fv)\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose if featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowing = True\n",
    "X_processed_sub1, Y_adjusted_sub1, groups_1 = process_input(X_sub1, Y_sub1, windowing)\n",
    "X_processed_sub2, Y_adjusted_sub2, groups_2 = process_input(X_sub2, Y_sub2, windowing)\n",
    "print(X_processed_sub1.shape)\n",
    "print(Y_adjusted_sub1.shape)\n",
    "\n",
    "print(X_processed_sub2.shape)\n",
    "print(Y_adjusted_sub2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_sub1 = scaler.fit_transform(X_processed_sub1) # this normalizes well\n",
    "X_sub2 = scaler.fit_transform(X_processed_sub2) # this normalizes well\n",
    "\n",
    "print(X_sub1.shape[1])\n",
    "for i in range(X_sub1.shape[1]):\n",
    "    sns.kdeplot(X_sub1[:,i])\n",
    "    \n",
    "print(X_sub2.shape[1])\n",
    "for i in range(X_sub2.shape[1]):\n",
    "    sns.kdeplot(X_sub2[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the variables to run the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1 = RandomForestClassifier(n_estimators=100)\n",
    "clf_2 = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = []\n",
    "scores_2 = []\n",
    "\n",
    "if windowing:\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "else:\n",
    "    cv = KFold(n_splits=10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on 1, testing on 2\n",
    "if windowing:\n",
    "    for train_index, test_index in cv.split(X_sub1, Y_adjusted_sub1, groups_1):\n",
    "        X_train, X_test, y_train, y_test = X_sub1[train_index], X_sub1[test_index], Y_adjusted_sub1[train_index], Y_adjusted_sub1[test_index]\n",
    "        clf_1.fit(X_train, y_train)\n",
    "        scores_1.append(clf_1.score(X_test, y_test))\n",
    "else:\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test, y_train, y_test = X_sub1[train_index], X_sub1[test_index], Y_adjusted_sub1[train_index], Y_adjusted_sub1[test_index]\n",
    "        clf_1.fit(X_train, y_train)\n",
    "        scores_1.append(clf_1.score(X_test, y_test))\n",
    "        \n",
    "        \n",
    "# training on 2, testing on 1      \n",
    "if windowing:\n",
    "    for train_index, test_index in cv.split(X_sub2, Y_adjusted_sub2, groups_2):\n",
    "        X_train, X_test, y_train, y_test = X_sub2[train_index], X_sub2[test_index], Y_adjusted_sub2[train_index], Y_adjusted_sub2[test_index]\n",
    "        clf_2.fit(X_train, y_train)\n",
    "        scores_2.append(clf_2.score(X_test, y_test))\n",
    "else:\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test, y_train, y_test = X_sub2[train_index], X_sub2[test_index], Y_adjusted_sub2[train_index], Y_adjusted_sub2[test_index]\n",
    "        clf_2.fit(X_train, y_train)\n",
    "        scores_2.append(clf_2.score(X_test, y_test))\n",
    " \n",
    "print(\"Mean accuracy testing on self, subject 1: \" + str(np.mean(scores_1)))\n",
    "print(\"Mean accuracy testing on self, subject 2: \" + str(np.mean(scores_2)))\n",
    "print(\"\")\n",
    "\n",
    "# testing subject 2 on subject 1 model\n",
    "print(\"Testing subject 2 on subject 1 model:\")\n",
    "print(\"Mean accuracy: \" + str(clf_1.score(X_sub2, Y_adjusted_sub2)))\n",
    "print(\"\")\n",
    "\n",
    "# testing subject 1 on subject 2 model\n",
    "print(\"Testing subject 1 on subject 2 model:\")\n",
    "print(\"Mean accuracy: \" + str(clf_2.score(X_sub1, Y_adjusted_sub1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "#     return ax\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y_sub1 = []\n",
    "pred_y_sub1 = []\n",
    "\n",
    "if windowing:\n",
    "    for train_index, test_index in cv.split(X_sub1, Y_adjusted_sub1, groups_1):\n",
    "        X_train, X_test, y_train, y_test = X_sub1[train_index], X_sub1[test_index], Y_adjusted_sub1[train_index], Y_adjusted_sub1[test_index]\n",
    "        \n",
    "        # get predictions\n",
    "        predictions = clf_1.fit(X_train, y_train).predict(X_test)\n",
    "        pred_y_sub1.extend(predictions)\n",
    "        true_y_sub1.extend(y_test)\n",
    "else:\n",
    "    for train_index, test_index in cv.split(X_sub1):\n",
    "        X_train, X_test, y_train, y_test = X_sub1[train_index], X_sub1[test_index], Y_adjusted_sub1[train_index], Y_adjusted_sub1[test_index]\n",
    "        \n",
    "        # get predictions\n",
    "        predictions = clf_1.fit(X_train, y_train).predict(X_test)\n",
    "        pred_y_sub1.extend(predictions)\n",
    "        true_y_sub1.extend(y_test)\n",
    "\n",
    "true_y_sub2 = []\n",
    "pred_y_sub2 = []\n",
    "if windowing:\n",
    "    for train_index, test_index in cv.split(X_sub2, Y_adjusted_sub2, groups_2):\n",
    "        X_train, X_test, y_train, y_test = X_sub2[train_index], X_sub2[test_index], Y_adjusted_sub2[train_index], Y_adjusted_sub2[test_index]\n",
    "        \n",
    "        # get predictions\n",
    "        predictions = clf_2.fit(X_train, y_train).predict(X_test)\n",
    "        pred_y_sub2.extend(predictions)\n",
    "        true_y_sub2.extend(y_test)\n",
    "else:\n",
    "    for train_index, test_index in cv.split(X_sub2):\n",
    "        X_train, X_test, y_train, y_test = X_sub2[train_index], X_sub2[test_index], Y_adjusted_sub2[train_index], Y_adjusted_sub2[test_index]\n",
    "        \n",
    "        # get predictions\n",
    "        predictions = clf_2.fit(X_train, y_train).predict(X_test)\n",
    "        pred_y_sub2.extend(predictions)\n",
    "        true_y_sub2.extend(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true y vs predicted y\n",
    "class_names = np.asarray([\"Tinder\", \"Instagram\", \"Notes\", \"Facebook\", \"LinkedIn\"])\n",
    "plot_confusion_matrix(true_y_sub1, pred_y_sub1, classes=class_names, normalize=True,\n",
    "                      title='Testing subject 1 on self')\n",
    "\n",
    "plot_confusion_matrix(true_y_sub2, pred_y_sub2, classes=class_names, normalize=True,\n",
    "                      title='Testing subject 2 on self')\n",
    "\n",
    "# testing on each other\n",
    "pred_res1 = clf_1.predict(X_sub2)\n",
    "plot_confusion_matrix(pred_res1, Y_adjusted_sub2, classes=class_names, normalize=True,\n",
    "                      title='Testing 2 on subject 1 model')\n",
    "\n",
    "pred_res2 = clf_2.predict(X_sub1)\n",
    "plot_confusion_matrix(pred_res2, Y_adjusted_sub1, classes=class_names, normalize=True,\n",
    "                      title='Testing 1 on subject 2 model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
