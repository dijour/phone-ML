{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Activity Recognition\n",
    "The goal is to develop a machine learning pipeline to recognize different social media usage using an iPhone’s motion sensors (accelerometer, gyroscope, etc).\n",
    "\n",
    "#### Activities\n",
    "1. Instagram\n",
    "2. Tinder\n",
    "3. Facebook\n",
    "4. LinkedIn\n",
    "5. Notetaking/texting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import numpy as np # used for handling numbers\n",
    "import scipy as sc\n",
    "import pandas as pd # used for handling the dataset\n",
    "import os # to get csv files of data\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, cross_val_score, train_test_split\n",
    "from sklearn.impute import SimpleImputer # used for handling missing data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # used for encoding categorical data\n",
    "from sklearn.preprocessing import StandardScaler # used for feature scaling\n",
    "\n",
    "# classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# neural networks\n",
    "import tensorflow as t # for RNN\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the sensor data from numpy files.\n",
    "### First, manually filter each CSV to remove any rows where Light > 100, to ensure that the data is only from when phone was in pocket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_folder, activity, convert_to_numpy=True):\n",
    "    out = []\n",
    "    file_names = os.listdir('%s/%s' %(base_folder, activity))\n",
    "    for file in file_names:\n",
    "        if (file == '.DS_Store'):\n",
    "            continue\n",
    "        data = pd.read_csv(('%s/%s/'+file) % (base_folder, activity),\n",
    "                            usecols=[\n",
    "                                    'loggingTime(txt)',\n",
    "                                    'loggingSample(N)', # this is easier to filter beginning/end frames\n",
    "                                    'locationLatitude(WGS84)',\n",
    "                                    'locationLongitude(WGS84)',\n",
    "                                    'locationAltitude(m)',\n",
    "                                    'locationSpeed(m/s)',\n",
    "#                                     'locationCourse(¬∞)',\n",
    "#                                     'locationHeadingX(¬µT)',\n",
    "#                                     'locationHeadingY(¬µT)',\n",
    "#                                     'locationHeadingZ(¬µT)',\n",
    "#                                     'locationTrueHeading(¬∞)',\n",
    "#                                     'locationMagneticHeading(¬∞)',\n",
    "                                    'accelerometerAccelerationX(G)',\n",
    "                                    'accelerometerAccelerationY(G)',\n",
    "                                    'accelerometerAccelerationZ(G)',\n",
    "                                    'gyroRotationX(rad/s)',\n",
    "                                    'gyroRotationY(rad/s)',\n",
    "                                    'gyroRotationZ(rad/s)',\n",
    "#                                     'magnetometerX(¬µT)',\n",
    "#                                     'magnetometerY(¬µT)',\n",
    "#                                     'magnetometerZ(¬µT)',\n",
    "                                    'motionYaw(rad)',\n",
    "                                    'motionRoll(rad)',\n",
    "                                    'motionPitch(rad)',\n",
    "                                    'motionRotationRateX(rad/s)',\n",
    "                                    'motionRotationRateY(rad/s)',\n",
    "                                    'motionRotationRateZ(rad/s)',\n",
    "                                    'motionUserAccelerationX(G)',\n",
    "                                    'motionUserAccelerationY(G)',\n",
    "                                    'motionUserAccelerationZ(G)',\n",
    "                                    'motionQuaternionX(R)',\n",
    "                                    'motionQuaternionY(R)',\n",
    "                                    'motionQuaternionZ(R)',\n",
    "                                    'motionQuaternionW(R)',\n",
    "                                    'motionGravityX(G)',\n",
    "                                    'motionGravityY(G)',\n",
    "                                    'motionGravityZ(G)',\n",
    "#                                     'motionMagneticFieldX(¬µT)',\n",
    "#                                     'motionMagneticFieldY(¬µT)',\n",
    "#                                     'motionMagneticFieldZ(¬µT)',\n",
    "                                    'altimeterRelativeAltitude(m)',\n",
    "                                    'altimeterPressure(kPa)',\n",
    "                                    'deviceOrientation(Z)',\n",
    "                                    'avAudioRecorderPeakPower(dB)',\n",
    "                                    'avAudioRecorderAveragePower(dB)']\n",
    "                          )\n",
    "        data = filter_input(data)\n",
    "        \n",
    "        if convert_to_numpy:\n",
    "            out.append(data.to_numpy())\n",
    "        else:\n",
    "            out.append(data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This particular cell of code was taken from:\n",
    "# https://github.com/KChen89/Accelerometer-Filtering/blob/master/acc.py\n",
    "#\n",
    "# This does median filtering to reduce noise for acc. and gyro data\n",
    "# Ignore this unless u want to understand implementation of plotting commented out in the next cell\n",
    "import math\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# creates a median filter on data\n",
    "def median_filter(data, f_size):\n",
    "    lgth, num_signal=data.shape\n",
    "    f_data=np.zeros([lgth, num_signal])\n",
    "    \n",
    "    for i in range(num_signal):\n",
    "        f_data[:,i]=signal.medfilt(data[:,i], f_size)\n",
    "    return f_data\n",
    "\n",
    "# creates a frequency filter on data\n",
    "def freq_filter(data, f_size, cutoff):\n",
    "    lgth, num_signal=data.shape\n",
    "    f_data=np.zeros([lgth, num_signal])\n",
    "    lpf=signal.firwin(f_size, cutoff, window='hamming')\n",
    "    for i in range(num_signal):\n",
    "        f_data[:,i]=signal.convolve(data[:,i], lpf, mode='same')\n",
    "    return f_data\n",
    "\n",
    "# creates a fast fourier transform plot on data\n",
    "def fft_plot(data, fs, title):\n",
    "    lgth, num_signal=data.shape\n",
    "    fqy=np.zeros([lgth,num_signal])\n",
    "    fqy[:,0]=np.abs(fft(data[:,0]))\n",
    "    fqy[:,1]=np.abs(fft(data[:,1]))\n",
    "    fqy[:,2]=np.abs(fft(data[:,2]))\n",
    "    index=np.arange(int(lgth/2))/(int(lgth/2)/(fs/2))\n",
    "    \n",
    "    fig, ax=plt.subplots()\n",
    "    labels=['x','y','z']\n",
    "    color_map=['r', 'g', 'b']\n",
    "    for i in range(3):\n",
    "        ax.plot(index, fqy[0:int(lgth/2),i], color_map[i], label=labels[i])\n",
    "    ax.set_xlim([0, fs/2])\n",
    "    ax.set_xlabel('Hz')\n",
    "    ax.set_title('Frequency spectrum: '+title) \n",
    "    ax.legend()\n",
    "\n",
    "# plots lines! Wooo\n",
    "def plot_lines(data, fs, title):\n",
    "    num_rows, num_cols=data.shape\n",
    "    if num_cols!=3:\n",
    "        raise ValueError('Not 3D data')\n",
    "        \n",
    "    fig, ax=plt.subplots()\n",
    "    labels=['x','y','z']\n",
    "    color_map=['r', 'g', 'b']\n",
    "    index=np.arange(num_rows)/fs\n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        ax.plot(index, data[:,i], color_map[i], label=labels[i])\n",
    "    ax.set_xlim([0,num_rows/fs])\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title('Time domain: '+title)\n",
    "    ax.legend()\n",
    "\n",
    "# creates the 3D plot of our accelerometer/gyroscope data\n",
    "def plot3D(data, title):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(xs=data[:,0], ys=data[:,1], zs=data[:,2], zdir='z')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters accelerometer or gyroscope data\n",
    "cutoff=10\n",
    "fs=512\n",
    "def filter_sensor(data, sensor_type):\n",
    "    if sensor_type == \"gyroscope\":\n",
    "        sensor_data = data.iloc[:,9:12].to_numpy()\n",
    "    else:\n",
    "        sensor_data = data.iloc[:,6:9].to_numpy()\n",
    "    \n",
    "#     median_data=median_filter(sensor_data, 155)\n",
    "    lpf_data=freq_filter(sensor_data, 155, cutoff/fs)\n",
    "#     comb_data=freq_filter(median_data, 155, cutoff/fs)\n",
    "    \n",
    "    if sensor_type == \"gyroscope\":\n",
    "        data.loc[:,'gyroRotationX(rad/s)'] = lpf_data[:,0]\n",
    "        data.loc[:,'gyroRotationY(rad/s)'] = lpf_data[:,1]\n",
    "        data.loc[:,'gyroRotationZ(rad/s)'] = lpf_data[:,2]\n",
    "    else:\n",
    "        data.loc[:,'accelerometerAccelerationX(G)'] = lpf_data[:,0]\n",
    "        data.loc[:,'accelerometerAccelerationY(G)'] = lpf_data[:,1]\n",
    "        data.loc[:,'accelerometerAccelerationZ(G)'] = lpf_data[:,2]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first 2 seconds (60 rows), and last 1 second (30 rows)\n",
    "def filter_input(data):\n",
    "    data = data.iloc[60:-60,]\n",
    "    data = filter_sensor(data, \"accelerometer\")\n",
    "    data = filter_sensor(data, \"gyroscope\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bennetthuffman/.local/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of X samples: 93\n",
      "Total number of Y samples: 93\n"
     ]
    }
   ],
   "source": [
    "# Load the data for each class\n",
    "X_tinder = load_data(\"data\", \"tinder\")\n",
    "X_instagram = load_data(\"data\", \"instagram\")\n",
    "X_notes = load_data(\"data\", \"notes\")\n",
    "X_facebook = load_data(\"data\", \"facebook\")\n",
    "X_linkedin = load_data(\"data\", \"linkedin\")\n",
    "\n",
    "# Assigning groundtruth conditions to each class\n",
    "Y_tinder = [0] * len(X_tinder) \n",
    "Y_instagram = [1] * len(X_instagram)\n",
    "Y_notes = [2] * len(X_notes)\n",
    "Y_facebook = [3] * len(X_facebook)\n",
    "Y_linkedin = [4] * len(X_linkedin)\n",
    "\n",
    "X = np.concatenate((X_tinder, X_instagram, X_notes, X_facebook, X_linkedin)) # insert standing when done \n",
    "Y = np.concatenate((Y_tinder, Y_instagram, Y_notes, Y_facebook, Y_linkedin)) # insert standing when done\n",
    "print(\"Total number of X samples: \" + str(len(X)))\n",
    "print(\"Total number of Y samples: \" + str(len(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook\n",
      "count    1936.000000\n",
      "mean       -0.017273\n",
      "std         0.018242\n",
      "min        -0.048404\n",
      "25%        -0.030939\n",
      "50%        -0.019925\n",
      "75%         0.000238\n",
      "max         0.019437\n",
      "Name: accelerometerAccelerationX(G), dtype: float64\n",
      "\n",
      "instagram\n",
      "count    1644.000000\n",
      "mean        0.057477\n",
      "std         0.052352\n",
      "min        -0.089476\n",
      "25%         0.037901\n",
      "50%         0.048763\n",
      "75%         0.058839\n",
      "max         0.316375\n",
      "Name: accelerometerAccelerationX(G), dtype: float64\n",
      "\n",
      "linkedin\n",
      "count    1512.000000\n",
      "mean        0.032837\n",
      "std         0.074381\n",
      "min        -0.067430\n",
      "25%        -0.016191\n",
      "50%         0.012497\n",
      "75%         0.053940\n",
      "max         0.347590\n",
      "Name: accelerometerAccelerationX(G), dtype: float64\n",
      "\n",
      "notes\n",
      "count    1654.000000\n",
      "mean       -0.001560\n",
      "std         0.042538\n",
      "min        -0.046114\n",
      "25%        -0.019441\n",
      "50%        -0.012825\n",
      "75%         0.006548\n",
      "max         0.244686\n",
      "Name: accelerometerAccelerationX(G), dtype: float64\n",
      "\n",
      "tinder\n",
      "count    1901.000000\n",
      "mean        0.068737\n",
      "std         0.040478\n",
      "min        -0.017575\n",
      "25%         0.036163\n",
      "50%         0.078413\n",
      "75%         0.099365\n",
      "max         0.139881\n",
      "Name: accelerometerAccelerationX(G), dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_name = 'accelerometerAccelerationX(G)' # replace me with other sensors!\n",
    "activities = ['facebook', 'instagram', 'linkedin', 'notes', 'tinder']\n",
    "\n",
    "for a in activities:\n",
    "    curr_data = load_data('samples', a, False)[0]\n",
    "    print(a)\n",
    "    print(curr_data[column_name].describe())\n",
    "    print('')\n",
    "    \n",
    "#     'accelerometerAccelerationX(G)',\n",
    "#     'accelerometerAccelerationY(G)',\n",
    "#     'accelerometerAccelerationZ(G)',\n",
    "#     'gyroRotationX(rad/s)',\n",
    "#     'gyroRotationY(rad/s)',\n",
    "#     'gyroRotationZ(rad/s)',\n",
    "#     'motionYaw(rad)',\n",
    "#     'motionRoll(rad)',\n",
    "#     'motionPitch(rad)',\n",
    "#     'motionRotationRateX(rad/s)',\n",
    "#     'motionRotationRateY(rad/s)',\n",
    "#     'motionRotationRateZ(rad/s)',\n",
    "#     'motionUserAccelerationX(G)',\n",
    "#     'motionUserAccelerationY(G)',\n",
    "#     'motionUserAccelerationZ(G)',\n",
    "#     'motionQuaternionX(R)',\n",
    "#     'motionQuaternionY(R)',\n",
    "#     'motionQuaternionZ(R)',\n",
    "#     'motionQuaternionW(R)',\n",
    "#     'motionGravityX(G)',\n",
    "#     'motionGravityY(G)',\n",
    "#     'motionGravityZ(G)',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the norm/magnitude of a 3-axis sensor\n",
    "def calc_sensor_norm(sensor_x, sensor_y, sensor_z):\n",
    "    sensor_total = []\n",
    "    for i in range(len(sensor_x)):\n",
    "        sensor_total.append((sensor_x[i]**2 + sensor_y[i]**2 + sensor_z[i]**2)**5)\n",
    "    return sensor_total\n",
    "\n",
    "# window input data by seconds and overlap\n",
    "def window_input(input_data, seconds=1, overlap=0.5, hz=30):\n",
    "    data_len = len(input_data)\n",
    "    window_len = seconds*hz\n",
    "    overlap_len = int(window_len * (1-overlap))\n",
    "    frame_count = 0\n",
    "    windows, next_win = [], []\n",
    "    # create windows if fits; up to overlap_len - 1 frames removed from end\n",
    "    for i in range(data_len):\n",
    "        next_win.append(input_data[i])\n",
    "        frame_count += 1\n",
    "        if (frame_count % overlap_len == 0) and (len(next_win) == window_len):\n",
    "            windows.append(next_win.copy())\n",
    "            del next_win[:overlap_len]\n",
    "    windows = np.asarray(windows)\n",
    "    return np.asarray(windows)\n",
    "\n",
    "# create spectrogram data. If windowing, calculate FFT at 50% overlap; else use whole recording\n",
    "def process_input(X, Y, windowing):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    groups = list([])\n",
    "    for i in range(len(X)):\n",
    "        if windowing:\n",
    "            windows = window_input(X[i], 3, 0.5, 30)\n",
    "            # featurize and add to output\n",
    "            for w in windows:\n",
    "                groups.append(i)\n",
    "                a_feat = featurize_input(w)\n",
    "                x_out.append(a_feat)\n",
    "                y_out.append(Y[i])\n",
    "        else:\n",
    "            groups.append(i)\n",
    "            w = featurize_input(X[i])\n",
    "            x_out.append(w)\n",
    "            y_out.append(Y[i])\n",
    "    return (np.asarray(x_out), np.asarray(y_out), np.asarray(groups))\n",
    "\n",
    "# Add bins as features or domain-specific features\n",
    "def featurize_input(sample):\n",
    "    fv = []\n",
    "    acc_x = sample[:,6]\n",
    "    acc_y = sample[:,7]\n",
    "    acc_z = sample[:,8]\n",
    "    acc_total = calc_sensor_norm(acc_x, acc_y, acc_z)\n",
    "    gyro_x = sample[:,9]\n",
    "    gyro_y = sample[:,10]\n",
    "    gyro_z = sample[:,11]\n",
    "    gyro_total = calc_sensor_norm(gyro_x, gyro_y, gyro_z)\n",
    "    loc_speed = sample[:,5]\n",
    "    altimeter = sample[:,28]\n",
    "    \n",
    "    motionYaw = sample[:,12]\n",
    "    motionRoll = sample[:,13]\n",
    "    motionPitch = sample[:,14]\n",
    "    motionRotationRateX = sample[:,15]\n",
    "    motionRotationRateY = sample[:,16]\n",
    "    motionRotationRateZ = sample[:,17]\n",
    "    \n",
    "#     'motionYaw(rad)',\n",
    "#     'motionRoll(rad)',\n",
    "#     'motionPitch(rad)',\n",
    "#     'motionRotationRateX(rad/s)',\n",
    "#     'motionRotationRateY(rad/s)',\n",
    "#     'motionRotationRateZ(rad/s)',\n",
    "#     'motionUserAccelerationX(G)',\n",
    "#     'motionUserAccelerationY(G)',\n",
    "#     'motionUserAccelerationZ(G)',\n",
    "    \n",
    "    fv = []\n",
    "    sample = np.asarray(acc_total)\n",
    "\n",
    "    # extremes\n",
    "    fv.append(np.max(acc_total))\n",
    "    fv.append(np.max(gyro_total))\n",
    "    fv.append(np.max(loc_speed))\n",
    "    fv.append(np.max(altimeter))\n",
    "    \n",
    "    # I noticed this helps w/ differentiating climbing\n",
    "    fv.append(np.max(gyro_x))\n",
    "    fv.append(np.max(gyro_y))\n",
    "    fv.append(np.max(gyro_z))\n",
    "\n",
    "    # averages\n",
    "    fv.append(np.mean(acc_total))\n",
    "    fv.append(np.mean(gyro_total))\n",
    "    fv.append(np.mean(loc_speed))\n",
    "    fv.append(np.mean(acc_x))\n",
    "    fv.append(np.mean(acc_y))\n",
    "    fv.append(np.mean(acc_z))\n",
    "    fv.append(np.mean(gyro_x))\n",
    "    fv.append(np.mean(gyro_y))\n",
    "    fv.append(np.mean(gyro_z))\n",
    "    \n",
    "    fv.append(np.median(acc_total))\n",
    "    fv.append(np.median(gyro_total))\n",
    "    fv.append(np.median(loc_speed))\n",
    "    \n",
    "\n",
    "    # deviation\n",
    "    fv.append(np.std(acc_total))\n",
    "    fv.append(np.std(gyro_total))\n",
    "    fv.append(np.std(loc_speed))\n",
    "    fv.append(np.std(altimeter))\n",
    "    \n",
    "    # Peak-to-peak\n",
    "    # Rootmean-square\n",
    "    # Correlation between values of accelerometer and gyroscope axes are extracted\n",
    "    \n",
    "    fv = np.asarray(fv)\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose if featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3838, 23)\n",
      "(3838,)\n"
     ]
    }
   ],
   "source": [
    "windowing = True\n",
    "X_processed, Y_adjusted, groups = process_input(X, Y, windowing)\n",
    "print(X_processed.shape)\n",
    "print(Y_adjusted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEFCAYAAADt1CyEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAUQklEQVR4nO3df5BdZ33f8ffHsowJJmCiLXEtYTmpJ8EE/IOtMIWCaYKRSWKlEzKVyw/DmGgGcH40TTt2OrWp+aO0mSYZYhOjgMbQBJvEQEZhZGxNIXVaENHaMcY/YlAMjaXxjDYWGKgJHplv/7hH9LLe1T27e3ev9Pj9mrmjc57nOed899j3s2fPOffcVBWSpHadMOkCJEkry6CXpMYZ9JLUOINekhpn0EtS406cdAHzWbduXW3cuHHSZUjScePOO+/8+6qamq/vmAz6jRs3MjMzM+kyJOm4keT/LNQ38tRNkg1JPpvk/iT3Jfm1ecYkyfuS7EtyT5Lzh/ouS/KV7nXZ0n8MSdJS9DmiPwz826q6K8mzgTuT7K6q+4fGXAyc1b1eBvwB8LIkzwOuAaaB6pbdWVVfH+tPIUla0Mgj+qp6pKru6qa/BTwAnD5n2BbgIzWwB3huktOA1wG7q+pQF+67gc1j/QkkSUe1qLtukmwEzgO+MKfrdODhofn9XdtC7fOte1uSmSQzs7OziylLknQUvYM+ySnAx4Ffr6pvjruQqtpeVdNVNT01Ne+FY0nSEvQK+iRrGYT8H1fVJ+YZcgDYMDS/vmtbqF2StEr63HUT4EPAA1X1OwsM2wm8pbv75gLgsap6BLgNuCjJqUlOBS7q2iRJq6TPXTevAN4MfCnJ3V3bbwEvAKiqG4BdwOuBfcDjwNu6vkNJ3gPs7Za7tqoOja98SdIoI4O+qv4XkBFjCnjXAn07gB1Lqm6Rfvn2XwbgDy/6w9XYnCQdF47JT8Yu1Z5H9ky6BEk65vhQM0lqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWrcyG+YSrID+DngYFX91Dz9/w5449D6XghMdd8X+zXgW8CTwOGqmh5X4ZKkfvoc0d8IbF6os6p+u6rOrapzgauA/znnC8Bf0/Ub8pI0ASODvqruAA6NGte5FLhpWRVJksZqbOfok/wQgyP/jw81F3B7kjuTbBux/LYkM0lmZmdnx1WWJD3tjfNi7M8D/3vOaZtXVtX5wMXAu5K8aqGFq2p7VU1X1fTU1NQYy5Kkp7dxBv1W5py2qaoD3b8HgU8Cm8a4PUlSDyPvuukjyXOAVwNvGmp7FnBCVX2rm74IuHYc21vIO6b+YSVXL0nHpT63V94EXAisS7IfuAZYC1BVN3TD/iVwe1X936FFnw98MsmR7Xy0qj49vtKf6idO/t5Krl6Sjksjg76qLu0x5kYGt2EOtz0EnLPUwiRJ4+EnYyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxI4M+yY4kB5Pcu0D/hUkeS3J397p6qG9zkgeT7Ety5TgLlyT10+eI/kZg84gxf1lV53avawGSrAGuBy4GzgYuTXL2coqVJC3eyKCvqjuAQ0tY9yZgX1U9VFVPADcDW5awHknSMozrHP3Lk3wxya1JXtS1nQ48PDRmf9c2ryTbkswkmZmdnR1TWZKkcQT9XcAZVXUO8PvAny1lJVW1vaqmq2p6ampqDGVJkmAMQV9V36yqb3fTu4C1SdYBB4ANQ0PXd22SpFW07KBP8qNJ0k1v6tb5KLAXOCvJmUlOArYCO5e7PUnS4pw4akCSm4ALgXVJ9gPXAGsBquoG4A3AO5IcBr4DbK2qAg4nuQK4DVgD7Kiq+1bkp5AkLWhk0FfVpSP6rwOuW6BvF7BraaVJksbBT8ZKUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWrcyKBPsiPJwST3LtD/xiT3JPlSks8lOWeo72td+91JZsZZuCSpnz5H9DcCm4/S/1Xg1VX1YuA9wPY5/a+pqnOranppJUqSlqPPl4PfkWTjUfo/NzS7B1i//LIkSeMy7nP0lwO3Ds0XcHuSO5NsO9qCSbYlmUkyMzs7O+ayJOnpa+QRfV9JXsMg6F851PzKqjqQ5B8Bu5P8TVXdMd/yVbWd7rTP9PR0jasuSXq6G8sRfZKXAB8EtlTVo0faq+pA9+9B4JPApnFsT5LU37KDPskLgE8Ab66qLw+1PyvJs49MAxcB8965I0laOSNP3SS5CbgQWJdkP3ANsBagqm4ArgZ+BHh/EoDD3R02zwc+2bWdCHy0qj69Aj+DJOko+tx1c+mI/rcDb5+n/SHgnKcuIUlaTX4yVpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrXK+iT7EhyMMm83/magfcl2ZfkniTnD/VdluQr3euycRUuSeqn7xH9jcDmo/RfDJzVvbYBfwCQ5HkMvmP2ZcAm4Jokpy61WEnS4vUK+qq6Azh0lCFbgI/UwB7guUlOA14H7K6qQ1X1dWA3R/+FIUkas3Gdoz8deHhofn/XtlC7JGmVHDMXY5NsSzKTZGZ2dnbS5UhSM8YV9AeADUPz67u2hdqfoqq2V9V0VU1PTU2NqSxJ0riCfifwlu7umwuAx6rqEeA24KIkp3YXYS/q2iRJq+TEPoOS3ARcCKxLsp/BnTRrAarqBmAX8HpgH/A48Lau71CS9wB7u1VdW1VHu6grSRqzXkFfVZeO6C/gXQv07QB2LL40SdI4HDMXYyVJK8Ogl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMb1Cvokm5M8mGRfkivn6f/dJHd3ry8n+cZQ35NDfTvHWbwkabSR3xmbZA1wPfBaYD+wN8nOqrr/yJiq+jdD438FOG9oFd+pqnPHV7IkaTH6HNFvAvZV1UNV9QRwM7DlKOMvBW4aR3GSpOXrE/SnAw8Pze/v2p4iyRnAmcBnhppPTjKTZE+SX1hoI0m2deNmZmdne5QlSepj3BdjtwK3VNWTQ21nVNU08K+B30vy4/MtWFXbq2q6qqanpqbGXJYkPX31CfoDwIah+fVd23y2Mue0TVUd6P59CPgLfvD8vSRphfUJ+r3AWUnOTHISgzB/yt0zSX4SOBX4/FDbqUme0U2vA14B3D93WUnSyhl5101VHU5yBXAbsAbYUVX3JbkWmKmqI6G/Fbi5qmpo8RcCH0jyPQa/VN47fLeOJGnljQx6gKraBeya03b1nPl3z7Pc54AXL6O+RfnPDEr66dXaoCQdB3oF/fHi3pwz6RIk6ZjjIxAkqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjWvqA1PPOfjeburmidYhSceSpoL+pH+4b9IlSNIxx1M3ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1rlfQJ9mc5MEk+5JcOU//W5PMJrm7e719qO+yJF/pXpeNs3hJ0mgjPzCVZA1wPfBaYD+wN8nOeb7k+2NVdcWcZZ8HXANMAwXc2S379bFUL0kaqc8R/SZgX1U9VFVPMHi+wJae638dsLuqDnXhvhvYvLRSJUlL0SfoTwceHprf37XN9YtJ7klyS5INi1yWJNuSzCSZmZ2d7VGWJKmPcV2M/XNgY1W9hMFR+4cXu4Kq2l5V01U1PTU1NaayJEl9gv4AsGFofn3X9n1V9WhVfbeb/SDw0r7LSpJWVp+g3wucleTMJCcBW4GdwwOSnDY0ewnwQDd9G3BRklOTnApc1LVJklbJyLtuqupwkisYBPQaYEdV3ZfkWmCmqnYCv5rkEuAwcAh4a7fsoSTvYfDLAuDaqjq0Aj+HJGkBvZ5HX1W7gF1z2q4emr4KuGqBZXcAO5ZRoyRpGfxkrCQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDWuV9An2ZzkwST7klw5T/9vJLk/yT1J/keSM4b6nkxyd/faOXdZSdLKGvlVgknWANcDrwX2A3uT7Kyq+4eG/TUwXVWPJ3kH8F+Bf9X1faeqzh1z3ZKknvoc0W8C9lXVQ1X1BHAzsGV4QFV9tqoe72b3AOvHW6Ykaan6BP3pwMND8/u7toVcDtw6NH9ykpkke5L8wkILJdnWjZuZnZ3tUZYkqY+Rp24WI8mbgGng1UPNZ1TVgSQ/BnwmyZeq6m/nLltV24HtANPT0zXOuiTp6azPEf0BYMPQ/Pqu7Qck+RngPwCXVNV3j7RX1YHu34eAvwDOW0a9kqRF6hP0e4GzkpyZ5CRgK/ADd88kOQ/4AIOQPzjUfmqSZ3TT64BXAMMXcSVJK2zkqZuqOpzkCuA2YA2wo6ruS3ItMFNVO4HfBk4B/jQJwN9V1SXAC4EPJPkeg18q751zt44kaYX1OkdfVbuAXXParh6a/pkFlvsc8OLlFChJWh4/GStJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXG9gj7J5iQPJtmX5Mp5+p+R5GNd/xeSbBzqu6prfzDJ68ZXuiSpj5FBn2QNcD1wMXA2cGmSs+cMuxz4elX9E+B3gf/SLXs2sBV4EbAZeH+3PknSKunz5eCbgH1V9RBAkpuBLcD9Q2O2AO/upm8BrkuSrv3mqvou8NUk+7r1fX485c/vlf/xIyu5+lV34gmHedbJ3550GVI7sgbWPpM1a36IZz7zjElX831n/+Mf5pqff9HY19sn6E8HHh6a3w+8bKExVXU4yWPAj3Tte+Yse/p8G0myDdjWzX47yYM9apvPOrjs75e47EpbB1jb4lnb4h2rdYG1HdW7F+4aVduCv7H6BP2qqKrtwPblrifJTFVNj6GksbO2pbG2xTtW6wJrW6rl1NbnYuwBYMPQ/Pqubd4xSU4EngM82nNZSdIK6hP0e4GzkpyZ5CQGF1d3zhmzE7ism34D8Jmqqq59a3dXzpnAWcBfjad0SVIfI0/ddOfcrwBuA9YAO6rqviTXAjNVtRP4EPDfu4uthxj8MqAb9ycMLtweBt5VVU+u0M9yxLJP/6wga1saa1u8Y7UusLalWnJtGRx4S5Ja5SdjJalxBr0kNe64DfrlPJZhwnW9Nclskru719tXo65u2zuSHExy7wL9SfK+rvZ7kpx/DNV2YZLHhvbb1atU14Ykn01yf5L7kvzaPGMmst961jap/XZykr9K8sWutv80z5hJvUf71Dax92m3/TVJ/jrJp+bpW/x+q6rj7sXgovDfAj8GnAR8ETh7zph3Ajd001uBjx0jdb0VuG5C++1VwPnAvQv0vx64FQhwAfCFY6i2C4FPTWCfnQac300/G/jyPP9NJ7LfetY2qf0W4JRuei3wBeCCOWNW/T26iNom9j7ttv8bwEfn+2+3lP12vB7Rf/+xDFX1BHDksQzDtgAf7qZvAX66eyzDpOuamKq6g8FdUQvZAnykBvYAz01y2jFS20RU1SNVdVc3/S3gAZ766e6J7LeetU1Ety+OPLdjbfeae+fHJN6jfWubmCTrgZ8FPrjAkEXvt+M16Od7LMPc/8F/4LEMwJHHMky6LoBf7P7EvyXJhnn6J6Vv/ZPy8u7P7VuTjP+BICN0fyKfx+AIcNjE99tRaoMJ7bfu9MPdwEFgd1UtuN9W8T3atzaY3Pv094B/D3xvgf5F77fjNeiPZ38ObKyqlwC7+f+/mXV0dwFnVNU5wO8Df7aaG09yCvBx4Ner6purue1RRtQ2sf1WVU9W1bkMPhG/KclPrda2R+lR20Tep0l+DjhYVXeOc73Ha9Av57EME62rqh6twdM8YfCn2UtXuKbFOGYfWVFV3zzy53ZV7QLWJlm3GttOspZBkP5xVX1iniET22+japvkfhuq4RvAZxk8qnzYJN6jvWqb4Pv0FcAlSb7G4NTvv0jyR3PGLHq/Ha9Bv5zHMky0rjnnbi9hcF71WLETeEt3F8kFwGNV9cikiwJI8qNHzkMm2cTg/90VD4Vumx8CHqiq31lg2ET2W5/aJrjfppI8t5t+JvBa4G/mDJvEe7RXbZN6n1bVVVW1vqo2MsiPz1TVm+YMW/R+O2aeXrkYtYzHMhwDdf1qkksYPBLiEIOr+6siyU0M7sJYl2Q/cA2DC1FU1Q3ALgZ3kOwDHgfedgzV9gbgHUkOA98Btq5GKDA4wnoz8KXunC7AbwEvGKptUvutT22T2m+nAR/O4IuGTgD+pKo+Nen36CJqm9j7dD7L3W8+AkGSGne8nrqRJPVk0EtS4wx6SWqcQS9JjTPoJWkVZMSD++aMfVWSu5IcTvKGefp/OMn+JNf12bZBL0mr40ae+qGxhfwdg1s6P7pA/3uAO/pu2KCXpFUw34P7kvx4kk8nuTPJXyb5yW7s16rqHuZ53k2SlwLPB27vu22DXpImZzvwK1X1UuA3gfcfbXCSE4D/1o3t7bj8ZKwkHe+6h9H9M+BPh54y/IwRi70T2FVV+xfzRGeDXpIm4wTgG91TNPt6OfDPk7wTOAU4Kcm3q+op32Y3d0OSpFXWPVL6q0l+Cb7/lZTnjFjmjVX1gu6hZ7/J4AtvjhryYNBL0qroHtz3eeAnulsjLwfeCFye5IvAfXTfSJfkn3YP9/sl4ANJ7lvWtn2omSS1zSN6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa9/8Ay0V2vCrRXA0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X_processed) # this normalizes well\n",
    "import seaborn as sns\n",
    "print(X.shape[1])\n",
    "for i in range(X.shape[1]):\n",
    "    sns.kdeplot(X[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the variables to run the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkn_clf = KNeighborsClassifier()\n",
    "nb_clf = GaussianNB()\n",
    "randf_clf = RandomForestClassifier()\n",
    "mlp_clf = MLPClassifier()\n",
    "logr_clf = LogisticRegression()\n",
    "svm_clf = SVC(gamma='auto', kernel='rbf')\n",
    "\n",
    "# pick one\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "if windowing:\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "else:\n",
    "    cv = KFold(n_splits=10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bennetthuffman/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:0.7001642623083223\n"
     ]
    }
   ],
   "source": [
    "# print(Y_adjusted) # y values, 0-5\n",
    "# print(groups) # group values, 0-119\n",
    "\n",
    "if windowing:\n",
    "    for train_index, test_index in cv.split(X, Y_adjusted, groups):\n",
    "#         print(\"Train Index: \", train_index)\n",
    "#         print(\"Test Index: \", test_index)\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y_adjusted[train_index], Y_adjusted[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_test, y_test))\n",
    "else:\n",
    "    for train_index, test_index in cv.split(X):\n",
    "#         print(\"Train Index: \", train_index)\n",
    "#         print(\"Test Index: \", test_index)\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y_adjusted[train_index], Y_adjusted[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print(\"Mean accuracy:\" + str(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate decision boundaries and spread of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give index of features that you are interested in looking at\n",
    "# mean, std, median, min, max, \n",
    "sub_features = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .2  # step size in the mesh\n",
    "x_min, x_max = X[:, 2].min() - .5, X[:, 2].max() + .5\n",
    "y_min, y_max = X[:, 3].min() - .5, X[:, 3].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "h = .2  # step size in the mesh\n",
    "x_min, x_max = X[:, sub_features[0]].min() - .5, X[:, sub_features[0]].max() + .5\n",
    "y_min, y_max = X[:, sub_features[1]].min() - .5, X[:, sub_features[1]].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the training points\n",
    "clf.fit(X_train[:,sub_features],y_train)\n",
    "scores.append(clf.score(X_test[:,sub_features], y_test))\n",
    "print('Mean Accuracy : ',str(np.mean(scores)))\n",
    "print('This accuracy number is not used for evaluating your solution as it only looks at a subset of features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n",
    "true_y = []\n",
    "pred_y = []\n",
    "\n",
    "if windowing:\n",
    "    for train_index, test_index in cv.split(X, Y_adjusted, groups):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y_adjusted[train_index], Y_adjusted[test_index]\n",
    "        \n",
    "        # get predictions\n",
    "        predictions = clf.fit(X_train, y_train).predict(X_test)\n",
    "        pred_y.extend(predictions)\n",
    "        true_y.extend(y_test)\n",
    "else:\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y_adjusted[train_index], Y_adjusted[test_index]\n",
    "        \n",
    "        # get predictions\n",
    "        predictions = clf.fit(X_train, y_train).predict(X_test)\n",
    "        pred_y.extend(predictions)\n",
    "        true_y.extend(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true y vs predicted y\n",
    "class_names = np.asarray([\"Tinder\", \"Instagram\", \"Notes\", \"Facebook\", \"LinkedIn\"])\n",
    "plot_confusion_matrix(true_y, pred_y, classes=class_names, normalize=True,\n",
    "                      title='Confusion matrix %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup\n",
    "\n",
    "This process was fairly difficult to first get started on. I first started by trying to visualize every class individually, just to get a sense for the data. It then took a little while to figure out the high level process, and what deliverables the assignment called for. Going to office hours was pretty useful and helped jumpstart my process.  \n",
    "\n",
    "Once I was able to sort the code out, it was clear that feature selection would be the single most effective factor in correct classification. Comparing windowing to non-windowing, it's clear that windowing the data is more effective in getting samples that have diverse characteristics. This requires the model to more accruately gauge acoustic qualia in the frequency domain. Unwindowed samples are naturally more obscure and require the computer to make decisions based on a whole array. In terms of practicality, it makes sense that windowing the samples is more ecologically sound and leads to higher performance - it's unlikely that audio samples in the real world can be heard in their entirety, without interruption or other noises. \n",
    "\n",
    "In terms of binning, it seems like binning could be useful in the event that contextual featurization is unintuitive or difficult to discern. Binning seems like a more elementary form of deep-learning, where the features are the data themselves, just more generalized and averaged across the bins. I prefer contextual featurization, since we had the added benefit of reading the provided papers where they provided insights for us on the best features to extract in the frequency and time domains. Had we not had this data, or the ability to research effective features, binning would be a viable tactic.\n",
    "\n",
    "I'm slightly concerned that my data is overfitted, as some of the performance metrics gathered seem too good to be true. However, I know that I correctly separated and grouped my data, so I don't know what other factors would lead to overfitting. I do know that spectral qualities like no. peaks and spectral centroid are powerful features. When I printed them out, it was pretty clear just from those two data points that each class was drastically different, by entire orders of magnitude in some cases. \n",
    "\n",
    "Overall, this was a pretty cool test of our abilities and I'm glad I got the chance to do this on my own data. Thanks for the opportunity! Please roast me and tell me how to make this better, as I'm frankly not entirely confident that I did this according to protocol. I'd love to improve and learn! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
